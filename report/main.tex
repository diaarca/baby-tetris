\documentclass[12pt, letterpaper]{article}
\usepackage[top=0.6in, bottom=0.6in, left=0.8in, right=0.8in, headheight=15pt, footskip=10pt]{geometry} % Ensure enough space for headers/footers
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{paracol}
\usepackage{vwcol}

\setlength{\parindent}{.45cm}
\pagestyle{plain} % Use plain page style for all pages

\newcommand{\code}[1]{\textbf{\textit{\texttt{#1}}}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{white},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\title{Baby Tetris First Part Report}
\author{ERGIN Seçkin Yağmur and GROUSSON Dylan}
\date{\today}

\begin{document}
\setcounter{page}{1} % Explicitly set the page number to start from 1
\maketitle
\thispagestyle{plain} % Ensure page numbers are shown on the title page

\begin{center}
    \textcolor{blue}{\url{https://github.com/diaarca/baby-tetris/}}
\end{center}

\tableofcontents

\section{Question 1: Discounted MDP For Player}
\label{sec:question1}
\quad The objective of this part is to choose the optimal policy that maximizes
the expected cumulative discounted reward over time for the "player".
{\renewcommand\labelitemi{}
\begin {itemize}
    \item \textbf{States}\\
        The state is represented by the current configuration of the grid, and
        the next piece to be placed.\\
        $S := (\text{current~grid},\ \text{next~piece~to~place})$
    \item \textbf{Actions}\\
        The actions correspond to a position and orientation to place the piece
        on the grid.\\
        $A := (\text{position},\ \text{orientation})$
    \item \textbf{Transition Function}\\
        The transition function is defined as
        $P(s' | s, a) :=  1/2$\\
        There are two possible pieces (I or L) as the next 
        piece to place, and considering our state representation, and our assumption 
        of uniform distribution between the pieces, each transition has an equal probability of $1/2$.
    \item \textbf{Reward Function and Discount Factor}\\
        The reward function is computed based on the number of lines cleared after placing the piece, 
        and a discount factor $\lambda$ is used to weigh future rewards.\\
        $R(s, a, s') := \text{coefficient * number~of~lines~cleared}$ with $\lambda \in [0, 1]$ 
\end{itemize} 
}

\subsection{Finding Optimal Policy}
\quad We use the value iteration method to find the optimal policy that maximizes 
the expected rewards, iteratively updating the value function for each state 
based on the Bellman equation until it converges to the optimal value (margin $\epsilon$).
\[
V(s) = \max_{a \in A} \sum_{s' \in S} P(s' | s, a) \cdot \left( R(s, a, s') + \lambda V(s') \right)
\]
Value iteration is better suited for maximizing the expected discounted reward 
since it computes the optimal value then extracts the policy and it's easier to implement.

\subsection{Implementation}
\subsubsection{Main Architecture}
\quad Based on the our design choices from Section~\ref{sec:question1}, we
decided to use an object oriented architecture (C++) for our implementation.
\par For our structure we have declared 7 classes:
\begin{enumerate}
    \item \code{Point}: classical point (x,y) class used for grid coordinates
    \item \code{Field}: contains a boolean grid s.t. grid[i][j] =
        1 if the corresponding cell is filled in the field
    \item \code{Tromino}: corresponds to the baby Tetris pieces
        (I Piece or L Piece)
    \item \code{State}: contains both a field grid and an
        incoming Tromino
    \item \code{Action}: represents a choosable action on a given state
        (combination for placement and rotation of an incoming Tromino)
    \item \code{Game}: contains the configuration (completed lines
        scores), the current state and score of a baby Tetris game (the
        configuration is loaded from a file : \code{config.txt})
    \item \code{MDP}: encompasses all the necessary methods in order to
        compute the value iteration over a game
\end{enumerate}

Now that we presented the overview of the global implementation architecture, we will describe
our implementation choices.
\subsubsection{Implementation Choices}
\begin{itemize}
    \item \textbf{Pieces teleported to their final position}: In order to reduce
        the number of intermediate states when choosing an action, we compute on
        a state all available actions, then according to the
        policy, we make a choice over these actions and we teleport the incoming
        piece in the current state directly to its final position (with the
        corresponding rotation)
    \item \textbf{Reward Function}: As defined in
        Section~\ref{sec:question1}, the reward function takes both the current state $s$ and an
        available action $a_s$. Since we can calculate the available
        actions, we know in which state $s'$ we're going to arrive and we
        can compute directly in $s'$ the number of completed lines before removing
        them from the game. Then we abstract the reward function by counting
        the number of complete lines in the resulting state
    \item \textbf{Game Ending}: According to the subject, the game should end
        whenever a piece exceeds the height limit of the grid.
        Since we compute all available actions on a given state, if a piece
        cannot be placed on the grid, there will be no available action for
        that state, the game will terminate when there is no available 
        action for the current state.
    \item \textbf{Max Iteration}: Since the value iteration isn't an exact
        method, if we fix an $\epsilon$ too small, we might iterate for a
        very long time. For this reason, we fix a maximum number of iteration
        for the expected value improvement to have a value iteration method
        that computes in reasonable time, even if the expected value vector is
        less qualitative than the $\epsilon$ margin we fixed
\end{itemize}

\subsection{Results}
\quad Here, we will present multiple examples of execution of our
implementation. In these executions we will use fixed values for:
$\text{width} = 4$, $\epsilon = 0.00000001$, $\text{maxIteration} = 100$,
$\text{probaIPiece} = 0.5$ and $\text{maxGameAction} = 10000$. Note that we
fixed a small $\epsilon$ in order to approach the optimal policy, 
keeping in mind that the algorithm is bounded in iteration by the $maxIteration$.
And the variant variables will be: the height $\in \{4, 5\}$ and the discount factor
$\lambda \in [0, 1]$.\\
\\
For each execution we will measure:
\begin{enumerate}
    \item the number of iterations for the value iteration algorithm
    \item the average expected gain of the optimal policy
    \item the final score either at the end of the game or when we reach the
        maximum number of actions
    \item the execution time of the said execution
\end{enumerate}

\newpage
\subsubsection{Executions}
\label{sec:executions}
\begin{paracol}{2}
    \begin{lstlisting}[title=height: 4 and $\lambda$: 0.1]
    All constants:
    width = 4, height = 4, probaIPiece = 0.5, maxGameAction = 10000
    epsilon = 1e-08, maxIteration = 100, lambda = 0.1

    i = 0 and delta = 6
    i = 1 and delta = 0.6
    i = 2 and delta = 0.06
    i = 3 and delta = 0.00525
    i = 4 and delta = 0.00045
    i = 5 and delta = 0

    average over final V 0.412926
    ....
    ....
    **..
    **..

    Game Over! Global score: 7849 in 10000 actions 
    make run  3.02s user 0.08s system 92% cpu 3.340 total
    \end{lstlisting}


  	\switchcolumn
    \begin{lstlisting}[title=height: 4 and $\lambda$: 0.4]
    All constants:
    width = 4, height = 4, probaIPiece = 0.5, maxGameAction = 10000
    epsilon = 1e-08, maxIteration = 100, lambda = 0.4

    i = 0 and delta = 6
    i = 1 and delta = 2.4
    i = 2 and delta = 0.96
    i = 3 and delta = 0.336
    i = 4 and delta = 0.1152
    i = 5 and delta = 0

    average over final V 0.473608
    ***.
    **..
    *.**
    *.**

    Game Over! Global score: 225 in 285 actions 
    make run  2.93s user 0.08s system 93% cpu 3.237 total
    \end{lstlisting}
\end{paracol}

\begin{paracol}{2}
    \begin{lstlisting}[title=height: 5 and $\lambda$: 0.1]
    All constants:
    width = 4, height = 5, probaIPiece = 0.5, maxGameAction = 10000
    epsilon = 1e-08, maxIteration = 100, lambda = 0.1

    i = 0 and delta = 6
    i = 1 and delta = 0.6
    i = 2 and delta = 0.06
    i = 3 and delta = 0.006
    i = 4 and delta = 0.00058125
    i = 5 and delta = 3.1875e-05
    i = 6 and delta = 0

    average over final V 0.462044
    ....
    ....
    ....
    ...*
    .***

    Game Over! Global score: 7864 in 10000 actions 
    make run  58.26s user 0.32s system 99% cpu 58.837 total
    \end{lstlisting}

    \switchcolumn
    \begin{lstlisting}[title=height: 5 and $\lambda$: 0.9]
    All constants:
    width = 4, height = 5, probaIPiece = 0.5, maxGameAction = 10000
    epsilon = 1e-08, maxIteration = 100, lambda = 0.9

    i = 0 and delta = 6
    i = 1 and delta = 5.4
    i = 2 and delta = 4.86
    i = 3 and delta = 4.374
    i = 4 and delta = 3.81358
    i = 5 and delta = 1.88219
    i = 6 and delta = 0

    average over final V 0.676855
    ....
    ....
    ....
    ..*.
    .***

    Game Over! Global score: 8513 in 10000 actions 
    make run  58.20s user 0.34s system 99% cpu 58.809 total
    \end{lstlisting}
\end{paracol}

\subsubsection{Observations}
\quad Over our executions, we remark that the $\lambda$ has a huge
impact on the optimal policy computed. On the smaller grid ($4
\times 4$), it is really difficult to reach the $maxGameAction$ with $\lambda =
0.4$, since the model will prioritize short term decisions in order to increase
its score faster. In comparison, with $\lambda = 0.1$, the model will not
gain that much per action, prioritizing safer actions that allow a higher score on
the long term.

That result was as expected, since in a $4 \times 4$ grid, the space for 
the risky moves is limited. The study of the game with the $4 \times 5$ grid 
shows that bigger grid provides a larger space to take risks in order to increase 
the global score. The game with $height = 5$ and $\lambda = 0.9$ illustrates this 
perfectly because it scores higher than the game with $height = 5$ and 
$\lambda = 0.1$, since the risky actions can be better expolited on a bigger grid.

In both cases, the total computational time is pretty acceptable (max 1 minute)
even if it can be largely optimized (see Section~\ref{sec:improvement}). About
the exact same improvement, the average expected gain $V$ cannot be really
discussed here since there is a huge part of states that aren't reachable in
practice.

Finally, we notice that the value iteration algorithm choice described in
Section~\ref{sec:question1} was pretty relevant because we can observe 
the quick convergence to the optimal policy (at the last iteration, $\Delta$ 
is always equal to $0$).

\subsubsection{Usage}
\quad Different constants can be adjusted by modifying the corresponding \code{\#define} 
statements in \code{src/Tetris.cpp}, \code{hdr/MDP.h} and \code{hdr/State.h}.

In order to recompile and run the whole project, execute the following
command:
\begin{lstlisting}
    make run
\end{lstlisting} 

Unlike the textual output shown in Section~\ref{sec:executions}, in the
submitted version of the code, there is an exhaustive description of the
played game (evolution of the state at each action) as follows:
\begin{lstlisting}
    ....                .*..                  .*..
    ..*.                ***.                  ***.
    ***. --- LPiece --> ***. -- Completion -> ***.
    ***.                ***.                  ***.
    Current score: 734

    .*..                .***                  ....
    ***.                ****                  .***
    ***. --- LPiece --> ***. -- Completion -> ***.
    ***.                ***.                  ***.
    Current score: 735

    ....                ***.                  ***.
    .***                .***                  .***
    ***. --- IPiece --> ***. -- Completion -> ***.
    ***.                ***.                  ***.
    Current score: 735
\end{lstlisting}

\subsection{Incoming Improvement}
\label{sec:improvement}
\quad For now we compute naively the value iteration algorithm on all state
combinations, which represents
\begin{align*}
    &= 2^{\text{gridHeight} * \text{gridWidth}} * \text{nbOfTrominoTypes} \\
    &= 2^{4*4} * 2 \\
    &= 2^{17} \\
    &= 131072
\end{align*}
states in the smallest ($4\times 4$) version of the game. In the near future we
want to reduce this number by computing the value iteration for only 
the reachable states (from a given starting state $s_0$). For this purpose,
we will make use of a Breadth First Search algorithm which will explore
iteratively all reachable states from the current one.

\end{document}
