\documentclass[12pt, letterpaper]{article}
\usepackage[top=0.3in, bottom=0.3in, left=0.8in, right=0.8in, headheight=15pt, footskip=10pt]{geometry} % Ensure enough space for headers/footers
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\setlength{\parindent}{2cm}
\pagestyle{plain} % Use plain page style for all pages

\newcommand{\code}[1]{\textbf{\textit{\texttt{#1}}}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\title{Baby Tetris First Part Report}
\author{ERGIN Seçkin Yağmur \\ GROUSSON Dylan}
\date{\today}

\begin{document}
\setcounter{page}{1} % Explicitly set the page number to start from 1
\maketitle
\thispagestyle{plain} % Ensure page numbers are shown on the title page
\section{Question 1: Discounted MDP For Player}
% The objective of this part is to choose the optimal policy for the "player" in the 
% Baby Tetris game as a discounted Markov Decision Process (MDP), with the goal of finding 
% a policy that maximizes the expected cumulative discounted reward over time.
The objective of this part is to choose the optimal policy that maximizes 
the expected cumulative discounted reward over time for the "player".
{\renewcommand\labelitemi{}
\begin {itemize} 
    \item \textbf{States}\\ 
        The state is represented by the current configuration of the grid, and the next 
        piece to be placed.\\
        $S := (\text{current~grid},\ \text{next~piece~to~place})$
    \item \textbf{Actions}\\
        The actions correspond to a position and orientation to place the piece on the grid.\\
        $A := (\text{position},\ \text{orientation})$
    \item \textbf{Transition Function}\\
        The transition function is defined as
        $P(s' | s, a) :=  1/2$\\
        There are two possible pieces (I or L) as the next 
        piece to place, and considering our state representation, and our assumption 
        of uniform distribution between the pieces, each transition has an equal probability of $1/2$.
    \item \textbf{Reward Function and Discount Factor}\\
        The reward function is defined as the number of lines cleared after placing the piece, 
        and a discount factor $\lambda$ is used to weigh future rewards.\\
        $R(s, a, s') := \text{number~of~lines~cleared}$ and $\lambda \in [0, 1]$ 
\end{itemize} 
}
\subsection{Finding Optimal Policy}
We use the value iteration method to find the optimal policy that maximizes 
the expected rewards, iteratively updating the value function for each state 
based on the Bellman equation until it converges to the optimal value (margin $\epsilon$).
\[
V(s) = \max_{a \in A} \sum_{s' \in S} P(s' | s, a) \cdot \left( R(s, a, s') + \lambda V(s') \right)
\]
Value iteration is better suited for maximizing the expected discounted reward 
since it computes the optimal value then extracts the policy and easier to implement
\section{Game Structure}
\end{document}
