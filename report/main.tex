\documentclass[12pt, letterpaper]{article}
\usepackage[top=0.6in, bottom=0.6in, left=0.8in, right=0.8in, headheight=15pt, footskip=10pt]{geometry} % Ensure enough space for headers/footers
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{paracol}
\usepackage{vwcol}
\usepackage{algorithm}
\usepackage{algpseudocode}

\setlength{\parindent}{.45cm}
\pagestyle{plain} % Use plain page style for all pages

\newcommand{\code}[1]{\textbf{\textit{\texttt{#1}}}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{white},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\title{Baby Tetris First Part Report}
\author{ERGIN Seçkin Yağmur and GROUSSON Dylan}
\date{\today}

\begin{document}
\setcounter{page}{1} % Explicitly set the page number to start from 1
\maketitle
\thispagestyle{plain} % Ensure page numbers are shown on the title page

\begin{center}
    \textcolor{blue}{\url{https://github.com/diaarca/baby-tetris/}}
\end{center}

\tableofcontents

\section{Question 1: Discounted MDP For Player}
\label{sec:question1}
\quad The objective of this part is to choose the optimal policy that maximizes
the expected cumulative discounted reward over time for the "player".
{\renewcommand\labelitemi{}
\begin {itemize}
    \item \textbf{States}\\
        The state is represented by the current configuration of the grid, and
        the next piece to be placed.\\
        $S := (\text{current~grid},\ \text{next~piece~to~place})$
    \item \textbf{Actions}\\
        The actions correspond to a position and orientation to place the piece
        on the grid.\\
        $A := (\text{position},\ \text{orientation})$
    \item \textbf{Transition Function}\\
        The transition function is defined as
        $P(s' | s, a) :=  1/2$\\
        There are two possible pieces (I or L) as the next 
        piece to place, and considering our state representation, and our assumption 
        of uniform distribution between the pieces, each transition has an equal probability of $1/2$.
    \item \textbf{Reward Function and Discount Factor}\\
        The reward function is computed based on the number of lines cleared after placing the piece, 
        and a discount factor $\lambda$ is used to weigh future rewards.\\
        $R(s, a, s') := \text{coefficient * number~of~lines~cleared}$ with $\lambda \in [0, 1]$ 
\end{itemize} 
}

\subsection{Finding Optimal Policy}
\quad We use the value iteration method to find the optimal policy that maximizes 
the expected rewards, iteratively updating the value function for each state 
based on the Bellman equation until it converges to the optimal value (margin $\epsilon$).
\[
V(s) = \max_{a \in A} \sum_{s' \in S} P(s' | s, a) \cdot \left( R(s, a, s') + \lambda V(s') \right)
\]
Value iteration is better suited for maximizing the expected discounted reward 
since it computes the optimal value then extracts the policy and it's easier to implement.

\subsection{Implementation}
\subsubsection{Main Architecture}
\quad Based on the our design choices from Section~\ref{sec:question1}, we
decided to use an object oriented architecture (C++) for our implementation.
\par For our structure we have declared 7 classes:
\begin{enumerate}
    \item \code{Point}: classical point (x,y) class used for grid coordinates
    \item \code{Field}: contains a boolean grid s.t. grid[i][j] =
        1 if the corresponding cell is filled in the field
    \item \code{Tromino}: corresponds to the baby Tetris pieces
        (I Piece or L Piece)
    \item \code{State}: contains both a field grid and an
        incoming Tromino
    \item \code{Action}: represents a choosable action on a given state
        (combination for placement and rotation of an incoming Tromino)
    \item \code{Game}: contains the configuration (completed lines
        scores), the current state and score of a baby Tetris game (the
        configuration is loaded from a file : \code{config.txt})
    \item \code{MDP}: encompasses all the necessary methods in order to
        compute the value iteration over a game
\end{enumerate}

Now that we presented the overview of the global implementation architecture, we will describe
our implementation choices.
\subsubsection{Implementation Choices}
\begin{itemize}
    \item \textbf{Pieces teleported to their final position}: In order to reduce
        the number of intermediate states when choosing an action, we compute on
        a state all available actions, then according to the
        policy, we make a choice over these actions and we teleport the incoming
        piece in the current state directly to its final position (with the
        corresponding rotation)
    \item \textbf{Reward Function}: As defined in
        Section~\ref{sec:question1}, the reward function takes both the current state $s$ and an
        available action $a_s$. Since we can calculate the available
        actions, we know in which state $s'$ we're going to arrive and we
        can compute directly in $s'$ the number of completed lines before removing
        them from the game. Then we abstract the reward function by counting
        the number of complete lines in the resulting state
    \item \textbf{Game Ending}: According to the subject, the game should end
        whenever a piece exceeds the height limit of the grid.
        Since we compute all available actions on a given state, if a piece
        cannot be placed on the grid, there will be no available action for
        that state, the game will terminate when there is no available 
        action for the current state.
    \item \textbf{Max Iteration}: Since the value iteration isn't an exact
        method, if we fix an $\epsilon$ too small, we might iterate for a
        very long time. For this reason, we fix a maximum number of iteration
        for the expected value improvement to have a value iteration method
        that computes in reasonable time, even if the expected value vector is
        less qualitative than the $\epsilon$ margin we fixed
\end{itemize}
\newpage
\subsubsection{Value Iteration Implementation For The Player}
\quad The value iteration algorithm is implemented for the player by maintaining a value function 
for each state and updating it based on the expected rewards from taking actions in that state. 
The output is a policy that maps each state to the action that maximizes the expected cumulative discounted reward.
\begin{lstlisting}[language=C++, caption={Lowest Maximal Expected Reward}, label={lst:minmax_diff_cpp}]
// ... (within the loop over all reachable states)
// for (auto& [currState, currValue] : V)
    Action& action : currState.getAvailableActions()
    for (int k = 0; k < nbActions; k++)
    {
        // rewards for action index k
        rewards[k] = 0.0;
        for (State& placedState : currState.genAllStatesFromAction(action))
        {
            State afterState = placedState.completeLines();
            double vAfter = V.at(afterState);
            double reward[k] = 0.5 * (placedState.evaluate(config_) + 
                                lambda * vAfter);
        }
    }
    vPrime = *std::max_element(rewards.begin(), rewards.end());
    
    for (int k = 0; k < nbActions; k++)
    {
        if (rewards[k] == vPrime)
        {
            // If we find a matching action, we add it to the policy
            A.insert_or_assign(currState.clone(), 
                                std::make_unique<Tromino>(k));
            break;
        }
    }
// ... (the rest of the loop updates delta and V[currState] with vPrime)
\end{lstlisting}
\subsection{Results}
\quad Here, we will present multiple examples of execution of our
implementation. In these executions we will use fixed values for:
$\text{width} = 4$, $\epsilon = 0.00000001$, $\text{maxIteration} = 100$,
$\text{probaIPiece} = 0.5$ and $\text{maxGameAction} = 10000$. Note that we
fixed a small $\epsilon$ in order to approach the optimal policy, 
keeping in mind that the algorithm is bounded in iteration by the $maxIteration$.
And the variant variables will be: the height $\in \{4, 5\}$ and the discount factor
$\lambda \in [0, 1]$.\\
\\
For each execution we will measure:
\begin{enumerate}
    \item the number of iterations for the value iteration algorithm
    \item the average expected gain of the optimal policy
    \item the final score either at the end of the game or when we reach the
        maximum number of actions
    \item the execution time of the said execution
\end{enumerate}

\newpage
\subsubsection{Executions}
\label{sec:executions}
\begin{paracol}{2}
    \begin{lstlisting}[title=height: 4 and $\lambda$: 0.1]
    All constants:
    width = 4, height = 4, probaIPiece = 0.5, maxGameAction = 10000
    epsilon = 1e-08, maxIteration = 100, lambda = 0.1

    i = 0 and delta = 6
    i = 1 and delta = 0.6
    i = 2 and delta = 0.06
    i = 3 and delta = 0.00525
    i = 4 and delta = 0.00045
    i = 5 and delta = 0

    average over final V 0.412926
    ....
    ....
    **..
    **..

    Game Over! Global score: 7849 in 10000 actions 
    make run  3.02s user 0.08s system 92% cpu 3.340 total
    \end{lstlisting}


  	\switchcolumn
    \begin{lstlisting}[title=height: 4 and $\lambda$: 0.4]
    All constants:
    width = 4, height = 4, probaIPiece = 0.5, maxGameAction = 10000
    epsilon = 1e-08, maxIteration = 100, lambda = 0.4

    i = 0 and delta = 6
    i = 1 and delta = 2.4
    i = 2 and delta = 0.96
    i = 3 and delta = 0.336
    i = 4 and delta = 0.1152
    i = 5 and delta = 0

    average over final V 0.473608
    ***.
    **..
    *.**
    *.**

    Game Over! Global score: 225 in 285 actions 
    make run  2.93s user 0.08s system 93% cpu 3.237 total
    \end{lstlisting}
\end{paracol}

\begin{paracol}{2}
    \begin{lstlisting}[title=height: 5 and $\lambda$: 0.1]
    All constants:
    width = 4, height = 5, probaIPiece = 0.5, maxGameAction = 10000
    epsilon = 1e-08, maxIteration = 100, lambda = 0.1

    i = 0 and delta = 6
    i = 1 and delta = 0.6
    i = 2 and delta = 0.06
    i = 3 and delta = 0.006
    i = 4 and delta = 0.00058125
    i = 5 and delta = 3.1875e-05
    i = 6 and delta = 0

    average over final V 0.462044
    ....
    ....
    ....
    ...*
    .***

    Game Over! Global score: 7864 in 10000 actions 
    make run  58.26s user 0.32s system 99% cpu 58.837 total
    \end{lstlisting}

    \switchcolumn
    \begin{lstlisting}[title=height: 5 and $\lambda$: 0.9]
    All constants:
    width = 4, height = 5, probaIPiece = 0.5, maxGameAction = 10000
    epsilon = 1e-08, maxIteration = 100, lambda = 0.9

    i = 0 and delta = 6
    i = 1 and delta = 5.4
    i = 2 and delta = 4.86
    i = 3 and delta = 4.374
    i = 4 and delta = 3.81358
    i = 5 and delta = 1.88219
    i = 6 and delta = 0

    average over final V 0.676855
    ....
    ....
    ....
    ..*.
    .***

    Game Over! Global score: 8513 in 10000 actions 
    make run  58.20s user 0.34s system 99% cpu 58.809 total
    \end{lstlisting}
\end{paracol}

\subsubsection{Observations}
\quad Over our executions, we remark that the $\lambda$ has a huge
impact on the optimal policy computed. On the smaller grid ($4
\times 4$), it is really difficult to reach the $maxGameAction$ with $\lambda =
0.4$, since the model will prioritize short term decisions in order to increase
its score faster. In comparison, with $\lambda = 0.1$, the model will not
gain that much per action, prioritizing safer actions that allow a higher score on
the long term.

That result was as expected, since in a $4 \times 4$ grid, the space for 
the risky moves is limited. The study of the game with the $4 \times 5$ grid 
shows that bigger grid provides a larger space to take risks in order to increase 
the global score. The game with $height = 5$ and $\lambda = 0.9$ illustrates this 
perfectly because it scores higher than the game with $height = 5$ and 
$\lambda = 0.1$, since the risky actions can be better expolited on a bigger grid.

In both cases, the total computational time is pretty acceptable (max 1 minute)
even if it can be largely optimized (see Section~\ref{sec:improvement}). About
the exact same improvement, the average expected gain $V$ cannot be really
discussed here since there is a huge part of states that aren't reachable in
practice.

Finally, we notice that the value iteration algorithm choice described in
Section~\ref{sec:question1} was pretty relevant because we can observe 
the quick convergence to the optimal policy (at the last iteration, $\Delta$ 
is always equal to $0$).

\subsection{Reachable States}
% \label{sec:improvement}
\quad Instead of computing the value iteration on all possible states, we focus on
the reachable states from a given starting state $s_0$. To achieve this, we
implement a Breadth First Search (BFS) algorithm that explores all reachable
states iteratively from the current state.:

\begin{algorithm}[H]
\caption{Generate Reachable States}
\begin{algorithmic}[1]
\State $Q$: a queue of states
\State $s$: a state
\State $H$: a hashMap of states
\State $s \gets s_0$
\State $Q.\text{push}(s)$
\While{$Q.\text{isNotEmpty}()$}
    \State $s \gets Q.\text{pop}()$
    \For{$a$: \text{availableActions}$(s)$}
        \State $s_{\text{Placed}} \gets \text{applyAction}(s, a)$
        \State $s_{\text{After}} \gets s_{\text{Placed}}.\text{completeLines}()$
        \If{$Q.\text{doesNotContain}(s_{\text{After}})$ \textbf{and} $H.\text{doesNotContain}(s_{\text{After}})$}
            \State $Q.\text{push}(s_{\text{After}})$
        \EndIf
    \EndFor
    \State $H.\text{push}(s)$
\EndWhile
\State \Return $H$
\end{algorithmic}
\end{algorithm}

\newpage

Then we can repeat our experiments that produces results that are way better:
\begin{paracol}{2}
    \begin{lstlisting}[title=height: 4 and $\lambda$: 0.1]
    All constants:
    width = 4, height = 4, probaIPiece = 0.5, maxGameAction = 10000, epsilon = 1e-08, maxIteration = 1000, lambda = 0.1

    i = 0 and delta = 6.10288
    i = 1 and delta = 0.352625
    i = 2 and delta = 0.0250445
    i = 3 and delta = 0.0010013
    i = 4 and delta = 4.28143e-05
    i = 5 and delta = 1.79071e-06
    i = 6 and delta = 4.8676e-08
    i = 7 and delta = 1.07541e-09

    average over final V 0.565842
    ....
    ....
    ..**
    ..**

    Game Over! Global score: 7632 in 10000 actions
    _________________________________
    Executed in    7.69 secs
    \end{lstlisting}


  	\switchcolumn
    \begin{lstlisting}[title=height: 4 and $\lambda$: 0.9]
    All constants:
    width = 4, height = 4, probaIPiece = 0.5, maxGameAction = 10000, epsilon = 1e-08, maxIteration = 1000, lambda = 0.9

    i = 0 and delta = 8.36976
    i = 1 and delta = 5.32365
    i = 2 and delta = 3.54512
    i = 3 and delta = 2.10366
    ...
    i = 87 and delta = 1.33224e-08
    i = 88 and delta = 1.0733e-08
    i = 89 and delta = 8.64692e-09

    average over final V 4.15109
    *...
    *..*
    *..*
    **.*

    Game Over! Global score: 11118 in 10000 actions
    _________________________________
    Executed in   26.53 secs
    \end{lstlisting}
\end{paracol}

\begin{paracol}{2}
    \begin{lstlisting}[title=height: 5 and $\lambda$: 0.1]
    All constants:
    width = 4, height = 5, probaIPiece = 0.5, maxGameAction = 10000, epsilon = 1e-08, maxIteration = 1000, lambda = 0.1

    i = 0 and delta = 6.20552
    i = 1 and delta = 0.477762
    i = 2 and delta = 0.0334845
    i = 3 and delta = 0.00181086
    i = 4 and delta = 6.29689e-05
    i = 5 and delta = 2.6382e-06
    i = 6 and delta = 1.32337e-07
    i = 7 and delta = 6.63914e-09

    average over final V 0.559813
    ....
    ....
    ....
    *...
    ***.

    Game Over! Global score: 7620 in 10000 actions
    ________________________________
    Executed in   46.73 secs
    \end{lstlisting}

    \switchcolumn
    \begin{lstlisting}[title=height: 5 and $\lambda$: 0.9]
    All constants:
    width = 4, height = 5, probaIPiece = 0.5, maxGameAction = 10000, epsilon = 1e-08, maxIteration = 1000, lambda = 0.9

    i = 0 and delta = 9.7563
    i = 1 and delta = 6.7369
    i = 2 and delta = 4.43912
    i = 3 and delta = 2.70431
    ...
    i = 88 and delta = 1.44415e-08
    i = 89 and delta = 1.16674e-08
    i = 90 and delta = 9.42616e-09

    average over final V 4.54237
    ...*
    ..**
    .***
    .***
    .***

    Game Over! Global score: 13305 in 10000 actions
    _________________________________
    Executed in  402.82 secs
    \end{lstlisting}
\end{paracol}

\quad We can then remark that with this new implementation, the value iteration algorithm is quite slower both in terms of computation (per iteration) and in number of iteration. For instance, the 4$\times$4 game with $\lambda = 0.4$ wasn't able to play all the 10000 actions with being blocked while we can now play it with a way more "risky" policy ($\lambda = 0.9$).

\section{Question 2: Adversarial MDP For Opponent}
\quad The goal of this part is to design an adversary policy that selects tromino pieces for the game
with the intention of minimizing the players score.
For this policy we use once again value iteration, but this time we minimize the expected cumulative 
discounted reward over time from the available actions. The policy chooses the tromino piece that 
leads to the lowest expected score for the player.
We have the same states (reachable states) and reward function as in Section~\ref{sec:question1}.
{\renewcommand\labelitemi{}
\begin {itemize}
    \item \textbf{Actions}\\
    The actions for the adversary correspond to the choice of the next tromino piece for the player to place on the grid.\\
    A := \{I, L\}
    \item \textbf{Transition Function}\\
    The transition function for the adversary is defined as :\\
    \[P(s' | s, a, t) := 1/|A_t(s)|\]
    where $A_t(s)$ is the set of available actions for the player given the current state $s$ and the chosen tromino piece $t$,  
    with the assumption of a uniform distribution over the player's possible actions for the given piece.
\end {itemize}
}
% \[
% V(s) = \min_{a \in A} \sum_{s' \in S} P(s' | s, a) \cdot \left( R(s, a, s') + \lambda V(s') \right)
% \]
The player will still use the optimal policy computed in Section~\ref{sec:question1} to play the game.

\subsubsection{Choosing the Tromino Piece}
\quad We considered two different heuristic approaches for choosing the tromino piece, their value iterations are defined as:
\begin{itemize}
    \item \textbf{Lowest Maximal Expected Reward}: The policy chooses the tromino piece that leads to the lowest maximal 
    expected reward for the player.
    \[
    V(s) = \min_{t \in \{I,L\}} \max_{a \in A_t(s)} \sum_{s'} P(s'|s,a,t) \cdot \left( R(s,a,s') + \lambda V(s') \right)
    \]

    \item \textbf{Lowest Average Expected Reward}: The policy chooses the tromino piece that leads to the lowest average
    expected reward for the player.
    \[
    V(s) = \min_{t \in \{I,L\}} \frac{1}{|A_t(s)|} \sum_{a \in A_t(s)} \sum_{s'} P(s'|s,a,t) \cdot \left( R(s,a,s') + \lambda V(s') \right)
    \]
\end{itemize}
Both approaches produce similar results, with no remarkable difference. However, compared to the random
adversary policy, where the tromino pieces are chosen randomly, the tromino value iteration, for both approaches, 
successfully decreases the player's score.  \\

% \subsection{Implementation}
% \quad We add a new method in the \code{MDP} class that computes value iteration for the adversary policy. Both the \code{trominoValueIterationMinMax} and \code{trominoValueIterationMinAvg} methods share a common value iteration framework. They primarily differ in how the next tromino piece is chosen by the adversary, which in turn defines the update rule for the state value function $V(s)$. The common part involves an outer loop for value iteration convergence and an inner loop iterating through all reachable states via a map `V`.

% \subsubsection{Lowest Maximal Expected Reward (\code{trominoValueIterationMinMax})}
% \quad In this approach, the adversary aims to minimize the maximum possible expected reward that the player can achieve. For each state, the algorithm computes the best outcome for the player for each potential next piece. The adversary then chooses the piece that leads to the lower of these two maximal rewards.

% \begin{lstlisting}[language=C++, caption={Lowest Maximal Expected Reward}, label={lst:minmax_diff_cpp}]
% // ... (within the loop over all reachable states)
% // for (auto& [currState, currValue] : V)

%     double maxI = 0.0, maxL = 0.0;
%     for (const Action& action : currState.getAvailableActions())
%     {
%         // This inner loop evaluates outcomes for both next-piece possibilities
%         for (State& placedState : currState.genAllStatesFromAction(action))
%         {
%             State afterState = placedState.completeLines();
%             double vAfter = V.at(afterState);
%             double reward = 0.5 * (placedState.evaluate(config_) + lambda * vAfter);

%             const Tromino* t = &afterState.getNextTromino();
%             if (dynamic_cast<const LPiece*>(t))
%             {
%                 maxL = std::max(maxL, reward);
%             }
%             else if (dynamic_cast<const IPiece*>(t))
%             {
%                 maxI = std::max(maxI, reward);
%             }
%         }
%     }
%     // The new state value (vPrime) is the minimum of the two maximal rewards
%     if (maxL < maxI)
%     {
%         T.insert_or_assign(currState.clone(), std::make_unique<LPiece>());
%         vPrime = maxL;
%     }
%     else
%     {
%         T.insert_or_assign(currState.clone(), std::make_unique<IPiece>());
%         vPrime = maxI;
%     }
% // ... (the rest of the loop updates delta and V[currState] with vPrime)
% \end{lstlisting}

% \subsubsection{Lowest Average Expected Reward (\code{trominoValueIterationMinAvg})}
% \quad In this variant, the adversary selects the next tromino ('I' or 'L') that results in the lowest average expected reward for the player, averaged over all available actions.

% \begin{lstlisting}[language=C++, caption={Lowest Average Expected Reward}, label={lst:minavg_diff_cpp}]
% // ... (within the loop over all reachable states)
% // for (auto& [currState, currValue] : V)

%     double avgI = 0.0, avgL = 0.0;
%     int nbActions = currState.getAvailableActions().size();

%     for (const Action& action : currState.getAvailableActions())
%     {
%         // This inner loop evaluates outcomes for both next-piece possibilities
%         for (State& placedState : currState.genAllStatesFromAction(action))
%         {
%             State afterState = placedState.completeLines();
%             double vAfter = V.at(afterState);
%             double reward = 0.5 * (placedState.evaluate(config_) + lambda * vAfter);

%             const Tromino* t = &afterState.getNextTromino();
%             if (dynamic_cast<const LPiece*>(t))
%             {
%                 avgL += reward;
%             }
%             else if (dynamic_cast<const IPiece*>(t))
%             {
%                 avgI += reward;
%             }
%         }
%     }
%     // Normalize the sums to get the average reward per action
%     if (nbActions > 0) {
%         avgI /= nbActions;
%         avgL /= nbActions;
%     }
%     // The new state value (vPrime) is the minimum of the two average rewards
%     if (avgL < avgI)
%     {
%         T.insert_or_assign(currState.clone(), std::make_unique<LPiece>());
%         vPrime = avgL;
%     }
%     else
%     {
%         T.insert_or_assign(currState.clone(), std::make_unique<IPiece>());
%         vPrime = avgI;
%     }
% // ... (the rest of the loop updates delta and V[currState] with vPrime)
% \end{lstlisting}

% \newpage

\subsection{Executions}
\quad According to our implementation, we made the same executions in order to compare our adversary policies. Here we play the value iteration policy (from Section~/ref{sec:question1}) for the player on 3 adversary policies: random (based on \code{PROBA\_I\_PIECE}), Min-Max value iteration and Min-Avg iteration:

\begin{paracol}{2}
    \begin{lstlisting}[title=height: 4 and $\lambda_{tromino}$: 0.1]
    All constants:
    width = 4, height = 4, probaIPiece = 0.5, maxGameAction = 10000, epsilon = 1e-08, maxIteration = 1000, action policy lambda = 0.9, tromino policy lambda = 0.1 and number of simulation = 50

    Average results:
    Random Adversary moves: 11086.3
    Min Max Adversary Moves: 9999
    Min Avg Adversary Moves: 10000
    ___________________________________
    Executed in   35.04 secs
    \end{lstlisting}


  	\switchcolumn
    \begin{lstlisting}[title=height: 4 and $\lambda_{tromino}$: 0.9]
    All constants:
    width = 4, height = 4, probaIPiece = 0.5, maxGameAction = 10000, epsilon = 1e-08, maxIteration = 1000, action policy lambda = 0.9, tromino policy lambda = 0.9 and number of simulation = 50

    Average results:
    Random Adversary moves: 11091.7
    Min Max Adversary Moves: 9999
    Min Avg Adversary Moves: 10000
    ___________________________________
    Executed in   40.04 secs
    \end{lstlisting}
\end{paracol}

\begin{paracol}{2}
    \begin{lstlisting}[title=height: 5 and $\lambda_{tromino}$: 0.1]
    All constants:
    width = 4, height = 5, probaIPiece = 0.5, maxGameAction = 10000, epsilon = 1e-08, maxIteration = 1000, action policy lambda = 0.9, tromino policy lambda = 0.9 and number of simulation = 50

    Average results:
    Random Adversary moves: 13285
    Min Max Adversary Moves: 9996
    Min Avg Adversary Moves: 9996
    ___________________________________
    Executed in  630.78 secs
    \end{lstlisting}

    \switchcolumn
    \begin{lstlisting}[title=height: 5 and $\lambda_{tromino}$: 0.9]
    All constants:
    width = 4, height = 5, probaIPiece = 0.5, maxGameAction = 10000, epsilon = 1e-08, maxIteration = 1000, action policy lambda = 0.9, tromino policy lambda = 0.1 and number of simulation = 50

    Average results:
    Random Adversary moves: 13292.5
    Min Max Adversary Moves: 9996
    Min Avg Adversary Moves: 9996
    ___________________________________
    Executed in  537.57 secs
    \end{lstlisting}
\end{paracol}

\subsubsection{Observations}
\quad We can first observe that the score is not that much reduced by the adversary policies even if it is more noticeable in the 4$\times$5 case. This is caused by the lack of possibilities, we can only choose between 2 pieces where the player may have to choose between way more actions.

Contrarily to the player value iteration we can remark that the $\lambda$ parameter doesn't affect the score of the game. This is due to the fact that the score is minimized in that case and then the factor with which we multiply the $V(s)$ will not change anything.

\section{Question 3: Robust Policy For Player}
\quad In this section, we study the case where the player doesn't make any
assumptions on the adversary policy, searching for a generally suitable policy instead
of optimizing against a specific one. We accept a lower maximum score if it elevates
the minimal score across all adversary policies.\\

\quad To this end, the first idea we had was to implement a Max-Min value iteration.
Player policy always considers the worst-case scenario against all adversary policies.
This is achieved by modifying the value iteration to maximizing the minimum expected reward
over all possible states resulting from the adversary's actions (tromino choices).
\[
V(s) = \max_{a \in A} \min_{t \in \{I,L\}} P(s' | s, a, t) \cdot \left( R(s, a, s') + \lambda V(s') \right)
\]

\quad The second approach we had was to parameterize the base player value iteration.
The idea is to use these parameters as weight in the reward function to adjust the importance of each factor.
\[R(s, a, s') := \text{wl} \cdot \text{completed\_lines}_{s'} -
                \text{wh} \cdot \text{max\_cell\_height}_{s'} +
                \text{ws} \cdot \text{score}_{s'} -
                \text{wg} \cdot \text{gaps}_{s'}
\]
Where \code{wl}, \code{wh}, \code{ws} and \code{wg} are respectively the weights for completed lines, 
the highest occupied cell's height, the produced score and the number of gaps obtained after the tromino placement.

\subsection{Executions}
BLABLABLA
\subsection{Observations}
BLABLABLA
\section{Usage}
\quad Different constants can be adjusted by modifying the corresponding \code{\#define}
statements in \code{src/Tetris.cpp}, \code{hdr/MDP.h} and \code{hdr/State.h}.

In order to recompile and run the whole project, execute the following
command:
\begin{lstlisting}
    make run
\end{lstlisting}

Unlike the textual output shown in Section~\ref{sec:executions}, in the
submitted version of the code, there is an exhaustive description of the
played game (evolution of the state at each action) as follows:
\begin{lstlisting}
    ....                .*..                  .*..
    ..*.                ***.                  ***.
    ***. --- LPiece --> ***. -- Completion -> ***.
    ***.                ***.                  ***.
    Current score: 734

    .*..                .***                  ....
    ***.                ****                  .***
    ***. --- LPiece --> ***. -- Completion -> ***.
    ***.                ***.                  ***.
    Current score: 735

    ....                ***.                  ***.
    .***                .***                  .***
    ***. --- IPiece --> ***. -- Completion -> ***.
    ***.                ***.                  ***.
    Current score: 735
\end{lstlisting}

\end{document}
